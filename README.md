# Visual Affordances: Enabling robots to understand object functionality

[[arXiv](...)]
[[webpage](...)]

## Table of Contents
1. [Related works](#related-works)
    1. [Surveys]()
    2. [Task-driven object detection and segmentation](#task-driven-detection)
    3. [Affordance classification](#affordance-classification)
    4. [Affordance detection and segmentation](#affordance-detection)
    5. [Affordance grounding](#affordance-grounding)
    6. [End-effector pose estimation and synthesis](#end-effector-pose)
2. [Contributing](#contributing)
3. [Credits](#credits)
4. [Enquiries, Question and Comments](#enquiries-question-and-comments)
5. [License](#license)

## Related works <a name="related-works"></a>

<details>
<summary>  Surveys <a name="surveys"></a></summary>

- [Visual affordance and function understanding](https://dl.acm.org/doi/10.1145/3446370)
- [A survey of visual affordance recognition based on deep learning](https://ieeexplore.ieee.org/document/10171410)

</details>

<details>
<summary> Task-driven object detection and segmentation <a name="task-driven-detection"></a></summary>

- [What object should i use? - Task driven object detection](https://arxiv.org/abs/1904.03000)
- [TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object Detection](https://arxiv.org/abs/2403.08108)
- [VLTP: Vision-Language Guided Token Pruning for Task-Oriented Segmentation](https://arxiv.org/abs/2409.08464)
- [TOIST: Task oriented instance segmentation transformer with noun-pronoun distillation](https://arxiv.org/abs/2210.10775)
- [CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection](https://arxiv.org/abs/2309.01093)

</details>

<details>
<summary> Affordance classification <a name="affordance-classification"></a></summary>

- [Visual object-action recognition: Inferring object affordances from human demonstration](https://www.sciencedirect.com/science/article/pii/S107731421000175X) 
- [Learning visual object categories for robot affordance prediction](https://journals.sagepub.com/doi/abs/10.1177/0278364909356602)
- [High-level object affordance recognition](https://ieeexplore.ieee.org/document/8812515)
- [Functional object descriptors for human activity modeling](https://ieeexplore.ieee.org/document/6630736)

</details>

### Affordance detection and segmentation <a name="affordance-detection"></a>

### Affordance grounding <a name="affordance-grounding"></a>

<details>
<summary> End-effector pose estimation and synthesis <a name="end-effector-pose"></a></summary>
- [Ganhand: Predicting human grasp affordances in multi-object scenes](https://openaccess.thecvf.com/content_CVPR_2020/html/Corona_GanHand_Predicting_Human_Grasp_Affordances_in_Multi-Object_Scenes_CVPR_2020_paper.html)
- [Affordance diffusion: Synthesizing hand-object interaction](https://arxiv.org/abs/2303.12538)

</details>

## Contributing <a name="contributing"></a>

If you find an error, if you want to suggest a new feature or a change, you can use the issues tab to raise an issue with the appropriate label. 

Complete and full updates can be found in [CHANGELOG.md](CHANGELOG.md). The file follows the guidelines of [https://keepachangelog.com/en/1.1.0/](https://keepachangelog.com/en/1.1.0/).


## Credits <a name="credits"></a>


```
```


## Enquiries, Question and Comments <a name="enquiries-question-and-comments"></a>

If you have any further enquiries, question, or comments, or you would like to file a bug report or a feature request, please use the Github issue tracker. 


## Licence <a name="license"></a>
This work is licensed under the MIT License.  To view a copy of this license, see [LICENSE](LICENSE).
