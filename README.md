# Visual Affordances: Enabling robots to understand object functionality

[[arXiv](...)]
[[webpage](...)]

## Table of Contents
1. [Related works](#related-works)
    1. [Surveys]()
    2. [Task-driven object detection and segmentation](#task-driven-detection)
    3. [Affordance classification](#affordance-classification)
    4. [Affordance detection and segmentation](#affordance-detection)
    5. [Affordance grounding](#affordance-grounding)
    6. [End-effector pose estimation and synthesis](#end-effector-pose)
2. [Contributing](#contributing)
3. [Credits](#credits)
4. [Enquiries, Question and Comments](#enquiries-question-and-comments)
5. [License](#license)

## Related works <a name="related-works"></a>

<details>
<summary>  Surveys <a name="surveys"></a></summary>

- [Visual affordance and function understanding](https://dl.acm.org/doi/10.1145/3446370)
- [A survey of visual affordance recognition based on deep learning](https://ieeexplore.ieee.org/document/10171410)

</details>

<details>
<summary> Task-driven object detection and segmentation <a name="task-driven-detection"></a></summary>

- [What object should i use? - Task driven object detection](https://arxiv.org/abs/1904.03000)
- [TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object Detection](https://arxiv.org/abs/2403.08108)
- [VLTP: Vision-Language Guided Token Pruning for Task-Oriented Segmentation](https://arxiv.org/abs/2409.08464)
- [TOIST: Task oriented instance segmentation transformer with noun-pronoun distillation](https://arxiv.org/abs/2210.10775)
- [CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection](https://arxiv.org/abs/2309.01093)

</details>

<details>
<summary> Affordance classification <a name="affordance-classification"></a></summary>

- [Visual object-action recognition: Inferring object affordances from human demonstration](https://www.sciencedirect.com/science/article/pii/S107731421000175X) 
- [Learning visual object categories for robot affordance prediction](https://journals.sagepub.com/doi/abs/10.1177/0278364909356602)
- [High-level object affordance recognition](https://ieeexplore.ieee.org/document/8812515)
- [Functional object descriptors for human activity modeling](https://ieeexplore.ieee.org/document/6630736)

</details>

<details>
<summary> Affordance detection and segmentation <a name="affordance-detection"></a></summary>

- [AffordanceNet: An end-to-end deep learning approach for object affordance detection]()
- [Bayesian deep learning for affordance segmentation in images]()
- [Learning affordance segmentation: An investigative study]()
- [Are standard object segmentation models sufficient for learning affordance segmentation?]()
- [Object affordance detection with boundary-preserving network for robotic manipulation task]()
- [A new semantic edge aware network for object affordance detection]()
- [Object-based affordances detection with convolutional neural networks and dense conditional random fields]()
- [Weakly supervised affordance detection]()
- [Adosmnet: a novel visual affordance detection network with object shape mask guided feature encoders]()
- [Detecting object affordances with convolutional neural networks]()
- [FPHA-Afford: A domain-specific benchmark dataset for occluded object affordance estimation in human-object-robot interaction]()
- [Affordance segmentation of hand-occluded containers from exocentric images]()
- [Dual affordance detection using an efficient attention convolutional neural network]()
- [Multi-scale fusion and global semantic encoding for affordance detection]()
- [Object affordance detection with relationship-aware network]()
- [Strap: Structured object affordance segmentation with point supervision]()
- [Segmenting object affordances: Reproducibility and sensitivity to scale]()

</details>

<details>
<summary> Affordance grounding <a name="affordance-grounding"></a></summary>

- Understanding 3d object interaction from a single image
- Locate: Localize and transfer object parts for weakly supervised affordance grounding
- One-shot transfer of affordance regions? affcorrs!
- Demo2vec: Reasoning object affordances from online video
- Grounded human-object interaction hotspots from video
- Oval-prompt: Open-vocabulary affordance localization for robot manipulation through LLM affordance-grounding
- What does clip know about peeling a banana?
- One-shot open affordance learning with foundation model
- Learning affordance grounding from exocentric image
- Affordancellm: Grounding affordance from vision language model

</details>

<details>

<summary> Grasping detection <a name="grasping detection"></a></summary>

</details>

<details>

<summary> End-effector pose estimation and synthesis <a name="end-effector-pose"></a></summary>

- [Ganhand: Predicting human grasp affordances in multi-object scenes](https://openaccess.thecvf.com/content_CVPR_2020/html/Corona_GanHand_Predicting_Human_Grasp_Affordances_in_Multi-Object_Scenes_CVPR_2020_paper.html)
- [Affordance diffusion: Synthesizing hand-object interaction](https://arxiv.org/abs/2303.12538)

</details>

## Contributing <a name="contributing"></a>

If you find an error, if you want to suggest a new feature or a change, you can use the issues tab to raise an issue with the appropriate label. 

Complete and full updates can be found in [CHANGELOG.md](CHANGELOG.md). The file follows the guidelines of [https://keepachangelog.com/en/1.1.0/](https://keepachangelog.com/en/1.1.0/).


## Credits <a name="credits"></a>


```
```


## Enquiries, Question and Comments <a name="enquiries-question-and-comments"></a>

If you have any further enquiries, question, or comments, or you would like to file a bug report or a feature request, please use the Github issue tracker. 


## Licence <a name="license"></a>
This work is licensed under the MIT License.  To view a copy of this license, see [LICENSE](LICENSE).
